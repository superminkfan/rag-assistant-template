# Поведение

## Взаимодействия

| Потребитель | Поставщик | Момент взаимодействия | Протокол передачи данных | Технология взаимодействия | Протокол шифрования | Электронная подпись |
|---|---|---|---|---|---|---|
| Узел кластера | Интерфейс Discovery SPI | Запуск кластера | Zookeeper | Топология «кольцо» | TLS версии 1.2 и новее | — |
| Узел кластера | Интерфейс Communication SPI | Обмен сообщениями в кластере | Бинарный протокол обмена | socket-соединения | TLS версии 1.2 и новее | — |
| Узел кластера | Компонент PACMAN (CFGA) продукта Platform V Backend | Старт приложения, исполнение приложения | HTTPS | REST | TLS версии 1.2 и новее | — |
| Приложения | Сокращенная версия Communication SPI | Обмен сообщениями в кластере | Communication SPI |  Сокращенная версия интерфейса | TLS версии 1.2 и новее | — |
| Администратор | Интерфейс Communication SPI | Управление кластером | Communication SPI | Веб-интерфейс, shell-скрипты | TLS версии 1.2 и новее | — |
| Пользователь | Хранилища данных | Работа кластера | JDBC/ODBC | Persistence | — | - |
| vault-agent sidecar | Secret Management System | Исполнение приложения | HTTPS | REST | TLS версии 1.2 и новее | — |
| Zabbix agent | Интерфейс Metrics Exporter SPI | Работа кластера | JMX | — | TLS версии 1.2 и новее | — |
| Platform V Corax (KFKA) | Platfrom V DataGrid (IGN) | Работа кластера | Бинарный протокол обмена | socket-соединения | TLS версии 1.2 и новее | — |

## Механизмы безопасности

В DataGrid имеется плагин безопасности, предназначенный для аутентификации и авторизации пользователей.

### Функциональность плагина

- двухфакторная аутентификация пользователей по логину и паролю и по сертификату (пароли хешируются по алгоритму `PBKDF2WithHmacSHA512`);
- утилиты управления пользователями и разрешениями (веб-интерфейс, командная строка), не требующие перезапуска кластера для применения изменений;
- отправка событий ИБ в Syslog и UDP Syslog: аутентификация, авторизация, изменение топологии кластера, действия администраторов;
- может получать учетные данные из SecMan в случае, если настроена интеграция.

Плагин безопасности реализует аутентификацию пользователей DataGrid и авторизацию их действий в кластере.

Учетные записи (УЗ) пользователей в плагине могут быть:

-   персональными;
-   технологическими.

Для **аутентификации** клиентских подключений используются логин, пароль и сертификат пользователя.

Логин и пароль передаются по-разному для разных видов подключений. Сертификат пользователя берется из SSL-соединения.

**Авторизация** подразумевает проверку прав пользователя на операции с кешами, сервисами, compute tasks, а также на «системные» операции, например, право на ввод серверного узла в кластер (актуально только для технологических УЗ). Полную структуру разрешений Platform V DataGrid вы можете посмотреть в [официальной документации Apache Ignite](https://ignite.apache.org/releases/latest/javadoc/org/apache/ignite/plugin/security/SecurityPermissionSet.html).

Администрирование пользователей и их разрешений выполняется **графической** или **консольной** утилитами, входящими в состав плагина.

Графическая утилита администрирования — это независимое Spring Boot приложение, запускаемое отдельно от кластера.

Пользователь подключается к нему из браузера через REST. Административные задачи в свою очередь передаются кластеру с плагином безопасности через подключение `GridClient`.

Консольная утилита администрирования — это скрипт `ise-user-control.bat/sh`, который передает административные задачи кластеру с плагином безопасности через подключение `GridClient`.

### Архитектура плагина

#### Модель безопасности DataGrid

Узел DataGrid выполняет следующие проверки безопасности:

-   аутентификация подключающегося субъекта безопасности. Субъектом может быть соседний узел, желающий войти в состав кластера, либо входящее клиентское подключение одного из видов:

    -   `IgniteClient`;
    -   `GridClient`;
    -   JDBC-подключение;
    -   ODBC-подключение;

-   авторизация разрешения перед выполнением какой-либо операции (например, при добавлении данных в кеш). Проверка происходит в контексте безопасности субъекта, от имени которого выполняется текущая операция. Разрешения могут относиться к двум категориям:

    -   разрешения, привязанные к объекту с определенным именем (или набору имен с общим префиксом, например `AccountService*`). Например, `CACHE_PUT` всегда использует имя кеша, а `SERVICE_DEPLOY` — имя сервиса;
    -   cистемные разрешения. Они не связаны с именами объектов. Например, разрешение `JOIN_AS_SERVER` — это разрешение на вход в кластер в качестве серверного узла.

:::{admonition} Примечание
:class: note

Некоторые разрешения могут использоваться как с именами объектов, так и на системном уровне, например, `CACHE_CREATE`.
:::

В местах проверок вызываются методы абстракции `IgniteSecurity`. Вместо реальных проверок DataGrid содержит только реализации-заглушки, но позволяет сконфигурировать процессор, которому передаются все операции аутентификации и авторизации. Плагин безопасности в составе DataGrid содержит реализацию такого процессора.

#### Структура процессора безопасности

Реализация процессора в плагине (`SecurityProcessorImpl`) агрегирует следующие сущности:

-   `SecurityAuthenticator` — аутентифицирует пользователя в широком смысле. В качестве пользователя может выступать другой узел DataGrid, либо пользователь, подключившийся через клиентское соединение.
-   `SecurityAuthorizer` — авторизует действия пользователя.
-   `SecurityDataProvider` – хранит и позволяет считывать следующие данные о пользователях:

    -   логин и пароль — для аутентификации;
    -   разрешения — для авторизации операций.

-   `SecurityCredentialsProvider` — предоставляет логин и пароль пользователя, от имени которого запущен текущий узел DataGrid, чтобы другие узлы кластера могли аутентифицировать и авторизовать попытку входа этого узла в кластер.

#### Ролевая модель

На практике удобно управлять правами пользователей не на уровне отдельных разрешений, а на уровне ролей, представляющих собой набор разрешений.

Например, набор разрешений, необходимых для работы сотрудника определенной должности можно поместить в отдельную роль, а затем предоставить эту роль всем сотрудникам с такой должностью. В плагине все операции, связанные с предоставлением и изъятием прав пользователей, выполняются через роли.

При администрировании ролям выдаются разрешения на конкретные действия в кластере, а затем пользователям присваивается некоторый набор ролей. Любое действие в кластере, разрешение на которое содержится хотя бы в одной роли пользователя, будет успешно авторизовано плагином безопасности.

Подробнее о ролевой модели читайте в разделе [«Ролевая модель в DataGrid» документа «Руководство по безопасности»](../../security-guide/md/authorisation.md).

### Детали реализации

#### `SecurityDataProvider`

Для хранения данных о пользователях и ролях реализация использует `distributed metastorage` — внутреннее распределенное хранилище DataGrid, данные в котором синхронизированы между узлами в кластере и недоступны внешним пользователям через общедоступный API DataGrid.

Записи плагина безопасности в `distributed metastorage` бывают двух видов:

-   данные пользователя: логин, хеш пароля, список имен ролей;
-   данные роли: имя роли, набор разрешений DataGrid.

Для аутентификации используются только записи первого вида, для авторизации – обоих видов (чтобы получить полный набор разрешений пользователя). При первом запуске кластера плагин инициализирует заранее определенные роли и пользователей. Подробнее о ролях и пользователях читайте в разделе [«Ролевая модель в DataGrid» документа «Руководство по безопасности»](../../security-guide/md/authorisation.md).

#### `SecurityCredentialsProvider`

Для указания собственного логина и пароля узла плагин использует [SecurityCredentialsBasicProvider](https://ignite.apache.org/releases/latest/javadoc/org/apache/ignite/plugin/security/SecurityCredentialsBasicProvider.html) – простую реализацию провайдера, определенную в DataGrid. Логин и пароль вносятся непосредственно в файл Spring-конфигурации DataGrid. Защита пароля осуществляется сторонними средствами, например, с помощью компонента PACMAN (CFGA) продукта Platform V Backend (#BD).

#### Администрирование

Плагин поддерживает следующие способы администрирования пользователей и ролей:

-   использование утилиты администрирования с Web UI. Она реализована в виде Spring Boot-приложения (`ui-*.jar`), входящего в сборку плагина (и DataGrid);
-   использование консольной утилиты администрирования. Она запускается скриптом `bin/ise-user-control.sh`, входящим в поставку DataGrid (начиная с версии 4.281.1). Данная утилита удобна для автоматизированных действий, например, в continuous delivery.

Администратор безопасности при добавлении нового пользователя (любым способом) может назначить ему роль с именем `SHOULD_CHANGE_PASSWORD`, которая обрабатывается особым образом: разрешения других ролей пользователя не имеют эффекта, пока он с помощью утилиты администрирования не установит себе новый пароль.

Кроме стандартного способа создания (логин + пароль + подтверждение пароля) утилиты администрирования поддерживают создание пользователя с указанием хеша пароля вместо самого пароля. Это позволяет новым пользователям задавать желаемый пароль заблаговременно и передавать его в безопасном виде администратору для внесения в базу пользователей плагина. Для получения `Salt` и `Salted`-хеша пароля используется исполняемый файл `security-core.jar`.

#### Сертификаты пользователей

Для обеспечения достаточного уровня безопасности рекомендуется наличие уникального сертификата для каждого узла (как серверного, так и клиентского) в хранилище ключей на узле (в том числе и приватный ключ).

Для работы с сертификатами используется шифрование TLS версии 1.2 и новее. Описание этого протокола шифрования приведено в разделе [«Сетевая безопасность»](../../security-guide/md/network-security.md) документа «Руководство по безопасности».

Подробнее о требованиях к TLS-сертификату написано в разделе [«Управление ключами и сертификатами»](../../security-guide/md/key-and-certificate-management.md) документа «Руководство по безопасности».

### Считывание секретов и сертификатов из Secret Management System

При включенной интеграции с Secret Management System:

1.  На старте узла считывается значение из token файла.
2.  Токен внутри файла обновляется, в файл записывается новое значение токена.
3.  Считываются секреты. Секреты и сертификаты не сохраняются на локальном диске узла.

## Прочие поведенческие механизмы

### Конфигурация и режимы работы кешей в Apache DataGrid

#### Введение

Кеш — это набор пар «ключ-значение», предназначенный для хранения и быстрого доступа к объектам (эквивалентно понятиям «схема» или «таблица» в БД). Он позволяет работать с объектами по ключу, выполнять SQL-запросы и предоставляет ряд других функций. Кеш, определенный на одном узле кластера, доступен на всех остальных узлах.

В данном разделе описываются конфигурация и режимы работы кешей в Apache DataGrid, а также стратегии кеширования.

#### Конфигурация кешей в Apache DataGrid

DataGrid предоставляет различные конфигурации кешей, позволяющие контролировать поведение кеша в зависимости от требований используемого приложения. Параметры кеша определяются классом `CacheConfiguration`, который передается на кластер при помощи метода `IgniteConfiguration.setCacheConfiguration()`. Данный класс определяет все параметры конфигурации, необходимые для запуска кеша в кластере.

:::{admonition} Примечание
:class: note

На одном кластере можно сконфигурировать несколько кешей с разными названиями.
:::

Кеши DataGrid можно сконфигурировать для выполнения следующих функций:

-   партиционирование и репликация;
-   определение политик для работы с утерянными партициями;
-   создание основных и резервных копий данных;
-   создание групп кешей;
-   создание шаблонов кешей.

В DataGrid выделяют несколько режимов работы кешей:

-   PARTITIONED;
-   REPLICATED;
-   LOCAL;
-   NEAR;

а также несколько стратегий кеширования:

-   Cache-aside;
-   Read-through;
-   Write-through;
-   Write behind.

##### PARTITIONED

Режим PARTITIONED является наиболее масштабируемым режимом работы распределенного кеша. Он используется по умолчанию.

В данном режиме набор данных разделяется на равные части (партиции). Все партиции равно распределяются между узлами-участниками, создавая единое большое распределенное хранилище данных. Такой подход позволяет занимать данными весь объем памяти (ОЗУ и диск) на всех узлах. То есть, чем больше узлов в кластере, тем большее количество данных можно хранить.

В отличие от режима REPLICATED, где обновления довольно затратны за счет необходимости обновления каждого узла в кластере, в режиме PARTITIONED затраты на обновления снижаются за счет того, что необходимо обновлять лишь по одному основному узлу (и, опционально, по одному или больше резервных узлов) для каждого ключа. При этом немного увеличиваются затраты на операции чтения (read) за счет хранения кеша только на определенных узлах.

Чтобы избежать излишних перемещений данных, всегда выполняйте доступ к данным исключительно с узла, на котором хранится кеш. Такой подход называется affinity collocation. Его рекомендуется использовать при работе с partitioned-кешами.

:::{admonition} Примечание
:class: note

Partitioned-кеши рекомендуется использовать при работе с большими объемами данных и при их частом обновлении.
:::

##### REPLICATED

В режиме REPLICATED все данные реплицируются (копируются) на каждый узел в кластере. Данный режим работы кеша позволяет значительно повысить доступность данных за счет их копирования на все узлы в кластере. Однако при обновлении данных на одном узле приходится обновлять их и на всех остальных узлах, что может привести к снижению производительности и масштабируемости.

В DataGrid реализация replicated-кешей схожа с реализацией partitioned-кешей (каждый ключ имеет основную копию данных, а на остальных узлах кластера хранятся резервные копии).

Поскольку те же данные хранятся на остальных узлах в кластере, размер replicated-кеша ограничивается объемом памяти узла (ОЗУ и диск).

Данный режим рекомендуется использовать в сценариях, где операций чтения (read) из кеша значительно больше, чем операций записи (write), а объемы данных при этом небольшие. Если ваша система производит чтение из кеша в течение более 80% рабочего времени, то рекомендуется использовать именно режим REPLICATED.

:::{admonition} Внимание
:class: danger

Replicated-кеши необходимо использовать при работе с небольшими объемами данных и нечастом их обновлении.
:::

##### NEAR

Partitioned и replicated-кеши также могут обрамляться «near-кешем», который является локальным кешем, хранящим наиболее актуальные данные или данные из heap-памяти, доступ к которым осуществлялся чаще всего.

У near-кеша, как и в случае с partitioned-кешами, существует возможность контроля его размера, а также управления его политиками вытеснения данных (eviction).

Near-кеши создаются непосредственно на клиентских узлах. Для этого необходимо передать класс `NearCacheConfiguration` в методы `Ignite.createNearCache(NearCacheConfiguration)` или `Ignite.getOrCreateNearCache(String, NearCacheConfiguration)`. Если необходимо одновременно динамически запустить распределенный кеш и создать для него near-кеш, то необходимо применить метод `Ignite.getOrCreateCache(CacheConfiguration, NearCacheConfiguration)`:

::::{md-tab-set}
:::{md-tab-item} XML
```xml
<bean class="org.apache.ignite.configuration.CacheConfiguration">
    <property name="name" value="myCache" />

    <property name="nearConfiguration">
        <bean class="org.apache.ignite.configuration.NearCacheConfiguration">
            <property name="nearEvictionPolicy">
                <bean class="org.apache.ignite.cache.eviction.lru.LruEvictionPolicy">
                    <property name="maxSize" value="100000"/>
                </bean>
            </property>
        </bean>
    </property>
</bean>
```
:::

:::{md-tab-item} Java
```java
// Создание конфигурации near-кеша для `myCache`.
NearCacheConfiguration<Integer, Integer> nearCfg =
    new NearCacheConfiguration<>();

// Для автоматического вытеснения данных из near-кеша при
// достижении ими размера в `100_000` используйте политику
// вытеснения LRU (LRU eviction policy).
nearCfg.setNearEvictionPolicyFactory(new LruEvictionPolicyFactory<>(100_000));

// Создание распределенного кеша на серверных узлах и
// near-кеша `myCache` на локальном узле.
IgniteCache<Integer, Integer> cache = ignite.getOrCreateCache(
    new CacheConfiguration<Integer, Integer>("myCache"), nearCfg);
```
:::
::::

:::{admonition} Примечание
:class: note

Обычно при использовании DataGrid со включенной функцией Affinity collocation near-кеши не используются, так как все данные доступны локально из partitioned-кеша. Однако существуют ситуации, когда отправка результатов вычислений на удаленные узлы невозможна. В таких случаях near-кеши могут значительно повысить масштабируемость и общую производительность приложения.
:::

**Поддержка транзакций**

Near-кеши полностью поддерживают транзакции и обновляются/денонсируются автоматически при изменении данных на серверах.

**Near-кеши на серверных узлах**

При доступе к данным из partitioned-кешей на сервере без Affinity collocation может возникнуть необходимость сконфигурировать near-кеши на серверных узлах, используя системную функцию `CacheConfiguration.setNearConfiguration(...)`.

##### Конфигурация Near-кеша

Параметры конфигурации описаны в документации класса `CacheConfiguration`. Для конфигурации near-кеша используются параметры конфигурации сервера. Например, если кеш сервера имеет заданное значение параметра `ExpiryPolicy`, данные в near-кеше будут терять актуальность в соответствии с установленным значением политики.

Методы, приведенные в таблице ниже, не унаследованы от конфигурации сервера. Они устанавливаются отдельно, при помощи использования класса `NearCacheConfiguration`.

| Метод | Описание | Значение по умолчанию |
|---|---|---|
| `setNearEvictionPolicy(CacheEvictionPolicy)` | Политика вытеснения (eviction) для near-кеша | Отсутствует |
| `setNearStartSize(int)` | Начальный размер near-кеша | 375,000 |

##### Группы кешей

Группа кешей — объединение из нескольких кешей, в пределах которого происходит обмен картами партиций (Partition Map Exchange) и другими внутренними структурами.

Такой обмен позволяет ускорить обработку событий топологии и сократить общий объем используемой памяти.

##### Стратегии кеширования

##### Cache-aside

При использовании стратегии кеширования cache-aside кеш развертывается отдельно от основного хранилища данных и может «не знать» о его существовании. За синхронизацию данных в этом случае отвечает приложение или процесс захвата изменения данных (CDC). Например, при обновлении какой-либо записи в основном хранилище данных новые значения копируются в кеш.

Данная стратегия прекрасно подходит для работы в сценарии, когда кешированные данные статичны и редко обновляются, либо когда между двумя локациями данных допускается некоторая задержка/несогласованность. Обычно подразумевается, что данные в кеше и основном хранилище станут согласованными сразу после того, как изменения будут полностью скопированы в кеш.

Если DataGrid развернут в конфигурации cache-aside, то native persistence можно использовать для наборов данных DataGrid. Native persistence позволяет пропустить затратную по времени фазу «разогрева» кеша. Кроме того, поскольку native persistence всегда хранит копию всех данных на диске, пользователь имеет возможность кеширования части записей в ОЗУ. Если необходимой записи в памяти нет, DataGrid прочитает ее с диска автоматически, независимо от используемого API (SQL, ключ-значение или scan queries).

Приложение обновляет кеш в следующих случаях:

-   поиск данных в кеше не дал результатов, и приложение запрашивает Persistence-хранилище данных;
-   приложение обновляет значения в Persistence-хранилище;
-   приложение вводит значение в Persistence-хранилище.

Стратегия cache-aside крайне полезна для ускорения доступа к данным, но требует написания шаблонного кода для поддержки двух источников данных: БД и кеша.

##### Read-Through и Write-Through

При использовании стратегий read-through и write-through приложение обращается не к Persistence-хранилищу, а к кешу. Persistence-хранилище обновляется из кеша.

Стратегии read-through и write-through позволяют работать исключительно с кешем. При использовании данных стратегий приложению не нужно обслуживать два источника данных, как в случае со стратегией cache-aside.

:::{admonition} Примечание
:class: note

Все операции read-through и write-through будут полноценными участниками всех кеш-транзакций и будут фиксироваться и откатываться единым пакетом.
:::

**Read-through**

При использовании стратегии read-through приложение обращается для поиска необходимого значения к кешу. Если значение присутствует в кеше, он возвращает это значение. Если значение отсутствует в кеше, кеш выполняет поиск данного значения по Persistence-хранилищу, находит его, обновляется и возвращает значение.

:::{admonition} Внимание
:class: danger

Данная стратегия работает только для `cache get`-операций. `SELECT`-запросы от DataGrid не считывают данные из сторонних хранилищ. Для исполнения `SELECT`-запросов данные должны быть предварительно загружены в кеши DataGrid из БД.
:::

**Write-through**

При использовании стратегии write-through для обновления значения приложение обновляет кеш, а кеш обновляет Persistence-хранилище. При этом скорость записи довольно низкая, поскольку поток записи ждет обновления кеша и Persistence-хранилища.

**Конфигурация**

Чтобы сконфигурировать read-through и write-through, используйте интерфейс `CacheStore` и установите значение `cacheStoreFactory`, а также системных функций `readThrough` и `writeThrough` класса `CacheConfiguration`:

::::{md-tab-set}
:::{md-tab-item} Java
```java
IgniteConfiguration cfg = new IgniteConfiguration();

CacheConfiguration<Long, Person> cacheCfg = new CacheConfiguration<>();

cacheCfg.setCacheStoreFactory(FactoryBuilder.factoryOf(MyPersonStore.class));
cacheCfg.setReadThrough(true);
cacheCfg.setWriteThrough(true);

cfg.setCacheConfiguration(cacheCfg);

// Запустите узел DataGrid.
Ignition.start(cfg);
```
:::

:::{md-tab-item} XML
```xml
<bean class="org.apache.ignite.configuration.IgniteConfiguration">
...
    <property name="cacheConfiguration">
    <list>
        <bean class="org.apache.ignite.configuration.CacheConfiguration">
        ...
        <property name="cacheStoreFactory">
            <bean class="javax.cache.configuration.FactoryBuilder" factory-method="factoryOf">
            <constructor-arg value="foo.bar.MyPersonStore"/>
            </bean>
        </property>
        <property name="readThrough" value="true"/>
        <property name="writeThrough"  value="true"/>
            </bean>
        </list>
    </property>
...
</bean>
```
:::
::::

##### Write-Behind

В простом режиме write-through каждая `put` и `remove`-операция кеша будет включать в себя соответствующий запрос к Persistence-хранилищу, поэтому общая продолжительность обновления кеша может быть довольно долгой. Кроме того, высокая интенсивность обновлений кеша может привести к крайне высокой нагрузке на хранилище.

Для таких случаев DataGrid предлагает опцию асинхронного обновления Persistence-хранилища, также известную как write-behind. Основной идеей данного подхода является сбор обновлений и их последующая асинхронная загрузка в соответствующую БД.

Увеличение производительности при использовании данной стратегии обеспечивается при помощи сохранения данных одним большим пакетом, а не отдельными частями, как при использовании write-through. Обновления одного и того же типа (`put` или `remove`) при использовании write-behind можно сгруппировать в один пакет. Например, последовательные `put`-операции (`key1`, `value1`), (`key2`, `value2`), (`key3`, `value3`) можно сгруппировать в одну операцию `CacheStore.putAll(...)`.

Непосредственно сохранение данных может быть вызвано:

-   Событиями времени (ограничение максимального времени, в течение которого данные могут находиться в очереди).
-   Событиями размера очереди (очередь очищается при достижении определенной точки).
-   Обоими событиями. В этом случае любое событие вызовет сохранение.

:::{admonition} Примечание
:class: note

Использование стратегии write-behind повышает производительность благодаря асинхронному обновлению. Однако это может привести к потенциальной потере согласованности данных, так как некоторые обновления могут быть утеряны из-за ошибок или сбоев узла.
:::

**Последовательность обновлений**

При использовании стратегии write-behind в хранилище будет записано только последнее обновление записи. Например, если запись в кеше с ключом `key1` последовательно обновляется значениями `value1`, `value2` и `value3`, то в Persistence-хранилище будет записан только последний запрос на сохранение (`key1`, `value3`).

**Конфигурация**

Стратегия кеширования write-behind включается конфигурационной функцией `CacheConfiguration.setWriteBehindEnabled(boolean)`.

Для включения и конфигурации write-behind используйте указанные ниже методы класса `cacheConfiguration`:

| Метод | Описание | Значение по умолчанию |
|---|---|---|
| `setWriteBehindEnabled(boolean)` | Задает признак включения write-behind | `false` |
| `setWriteBehindFlushSize(int)` | Максимальный размер write-behind-кеша. Если размер кеша превышает данное значение, то все объекты кеша загружаются в `CacheStore`, и кеш записи очищается. Если данное значение равно 0, то загрузка производится в соответствии со значением интервала загрузки. Размер загружаемого кеша и частоты загрузки не могут быть одновременно равны 0 | 10240 |
| `setWriteBehindFlushFrequency(long)` | Частота загрузки write-behind-кеша в `CacheStore` в миллисекундах. Данное значение определяет максимальный временной интервал между внесением объекта в кеш или его удалением из него и моментом, когда соответствующая операция будет отправлена в `CacheStore`. Если данное значение равно 0, то загрузка производится в соответствии со значением размера загрузки. Размер загружаемого кеша и частоты загрузки не могут быть одновременно равны 0 | 5000 миллисекунд |
| `setWriteBehindFlushThreadCount(int)` | Количество потоков, производящих очистку кеша | 1 |
| `setWriteBehindBatchSize(int)` | Максимальный размер пакета для операций `CacheStore` при использовании write-behind | 512 |

## SQL-движок, основанный на Apache Calcite

:::{admonition} Внимание
:class: danger

Новый SQL-движок находится в статусе beta.
:::

### Введение

Начиная с версии 4.2130 DataGrid поставляется с новым SQL-движком, основанным на фреймворке Apache Calcite.

Apache Calcite — это фреймворк, позволяющий добавить функциональность выполнения SQL-запросов над хранилищем данных. Он служит в качестве посредника между приложениями, одним или несколькими хранилищами данных и движками обработки данных.

Текущий SQL-движок, основанный на H2, имеет определенный набор фундаментальных ограничений, связанных с исполнением SQL-запросов в распределенной среде. Новый движок позволяет обойти эти ограничения. Новый SQL-движок использует инструменты Apache Calcite для планирования и обработки запросов, а также новый процесс исполнения запросов.

### Библиотеки модуля Calcite

Для использования SQL-движка, основанного на Calcite, убедитесь в том, что библиотеки модуля Calcite находятся в classpath.

:::{admonition} Внимание
:class: danger

В настоящий момент часть функциональности модуля `ignite-indexing` также используется новым движком. Это значит, что модуль `ignite-indexing` также должен быть прописан в classpath.
:::

### Конфигурация

Способы конфигурирования SQL движка на основе Apache Calcite содержатся в документе «Руководство по системному администрированию».

#### Фазы выполнения запроса через Calcite

1.  Обработка:

    -   вход: строка самого запроса;
    -   выход: синтаксическое дерево (AST — Abstract Syntax Tree).

2.  Валидация (семантический анализ):

    -   вход: синтаксическое дерево (AST) и метаданные. На данном этапе синтаксическое дерево проверяется на соответствие метаданным;
    -   выход: AST с привязкой к конкретным метаданным.

3.  Построение логического плана запроса на основе AST:

    -   вход: AST;
    -   выход: логический план запроса (дерево реляционных операторов).

4.  Оптимизация:

    -   вход: логический план запроса и статистика;
    -   выход: физический план запроса (дерево реляционных операторов с привязкой к конкретному способу выполнения запроса)

5.  Выполнение:

    -   вход: физический план запроса;
    -   выход: результат выполнения (курсор).

### Согласованность данных

DataGrid — это строго согласованная платформа, реализующая двухфазный протокол фиксации. Гарантии согласованности выполняются как для уровней памяти, так и для дисков. Транзакции в DataGrid совместимы с ACID и могут охватывать несколько узлов и кешей кластера. База данных поддерживает pessimistic- и optimistic-режимы, взаимоблокировки (deadlock) и бездействия (deadlock-free).

В сценариях, где гарантии транзакций не являются обязательными, DataGrid позволяет выполнять запросы в atomic-режиме, что обеспечивает лучшую производительность.

#### Архитектура транзакций в DataGrid

Транзакцией называется группа из нескольких кеш-операций, объединенных в одну логическую операцию.

Частичное выполнение такой операции невозможно: все операции внутри транзакции должны быть выполнены, иначе произойдет откат транзакции.

##### Протокол двухфазной фиксации изменений (Two-Phase Commit Protocol)

В распределенных системах одна транзакция может затрагивать данные, расположенные на разных узлах, что накладывает дополнительные требования к обеспечению согласованности данных. Например, необходимо отслеживать и корректно обрабатывать ситуации, когда изменения применяются лишь одной частью узлов, если другая часть выходит из обслуживания либо «теряется».

Традиционным алгоритмом для решения таких задач выступает протокол двухфазной фиксации изменений (Two-Phase Commit Protocol).

Протокол имеет две фазы выполнения.

Первая фаза:

1.  Координатор транзакции (NearNode или приложение, запускающее транзакцию) отправляет prepare-сообщение всем Primary-узлам, участвующим в транзакции.
2.  Primary-узел пересылает сообщение узлам, на которых хранится резервная (Backup) копия данных (если такие узлы имеются) и применяет все необходимые блокировки.
3.  Primary-узел отправляет сообщение о применении всех блокировок и готовности к участию в транзакции.

Вторая фаза:

1.  Координатор, получив подтверждение от всех участников, отправляет сообщение «Commit» на primary-партиции, которые затем пересылают это сообщение резервным партициям.
2.  После получения сообщения узел фиксирует транзакцию, отправляя сообщение «ACK».

Такой метод гарантирует, что все участвующие узлы одновременно сохранят или отметят изменения транзакции.

Ниже рассматривается, как данный протокол переживает сбои, определяет тип транзакции (оптимистическая или пессимистическая) и прочие аспекты его работы.

**NearNode, RemoteNode и DHT**

NearNode — узел, выступающий в алгоритме двухфазной транзакции в роли координатора. С точки зрения DataGrid, NearNode — это узел, на котором была начата транзакция и был вызван `tx.start`. Такой узел отслеживает:

-   состояние транзакции;
-   затронутые ею ключи;
-   версию топологии старта транзакции;
-   узлы, участвующие в транзакции;
-   другие атрибуты контекста транзакции.

RemoteNode — узел, участвующий в транзакции и хранящий часть кеша, к которому осуществляется доступ или который обновляется в ходе транзакции. Физически каждый серверный узел размещает у себя часть DHT (распределенной хеш-таблицы), и (через Affinity Function) любой участник транзакции может определить партиции и узлы кластера, включая узел-координатор.

:::{admonition} Примечание
:class: note

DHT хранит контейнеры до уровня партиций, а дальше используется B+ tree.
:::

##### Режимы блокировки и уровни изоляции

В многопользовательских системах разные пользователи могут одновременно модифицировать одни и те же данные.

Для корректного разрешения таких ситуаций существует два основных подхода — пессимистическая и оптимистическая блокировка.

Также для блокировок важен уровень изоляции транзакции.

##### Пессимистическая блокировка

При пессимистической блокировке происходит следующее:

1.  Система блокирует данные, запланированные ей к изменению.
2.  Система вычисляет новые значения и гарантированно изменяет данные.

:::{admonition} Примечание
:class: note

При пессимистической блокировке данные остаются заблокированными до окончания транзакции. Для сокращения времени удержания блокировок может использоваться режим оптимистической блокировки.
:::

**Уровни изоляции при пессимистической блокировке**

При пессимистической блокировке существует несколько уровней изоляции:

-   **Read committed**. На данном уровне блокировки применяются перед операцией записи (`put`, `put_all`). Блокируются пересекающиеся по ключам `put`-операции.
-   **Repeatable read и Serializable**. На данном уровне блокировки применяются перед любой операцией (и чтения, и записи). Блокируются пересекающиеся по ключам `put` и `get`-операции.

:::{admonition} Примечание
:class: note

Количество пересекающихся по ключам транзакций в режиме пессимистической блокировки, отправленных из тонкого клиента, не может превышать размер пула потоков коннектора серверного узла (`ClientConnectorConfiguration#setThreadPoolSize`).

Транзакции завершаются ошибкой по тайм-ауту или не завершаются при конфигурации по умолчанию (без тайм-аута).

Данное ограничение является архитектурной особенностью.
:::

##### Оптимистическая блокировка

При оптимистической блокировке вначале вычисляется новое значение, затем проверяется, что за время вычислений не было внесено конкурирующих изменений:

-   если конкурирующих изменений внесено не было, данные блокируются и модифицируются;
-   если было обновление, необходимо провести вычисление еще раз.

##### Уровни изоляции при оптимистической блокировке

Здесь так же, как и в режиме пессимистической блокировки, существует несколько уровней изоляции:

-   **Read committed и Repeatable read.** На данном уровне изоляции блокировки приобретаются в момент выполнении фазы prepare. При этом проверка неизменности версии с начала транзакции не выполняется.
-   **Serializable.** На данном уровне изоляции блокировки также создаются в фазе prepare, но проверка неизменности версии выполняется.

##### Жизненный цикл транзакции

В данном разделе будет рассмотрен жизненный цикл транзакции в DataGrid.

Для простоты изложения предположим, что топология кластера стабильна и не меняется.

Началом транзакции является операция ее запуска `tx.start`. При этом, на координаторе транзакции (NearNode), создается структура для управления контекстом транзакции (интерфейс `IgniteInternalTx`).

На данном этапе для транзакции выполняются следующие операции:

1.  Генерируется уникальный идентификатор транзакции.
2.  Запоминается время начала транзакции.
3.  Запоминается актуальная версия топологии.
4.  Запоминается уровень изоляции и конкуренции, определенный для транзакции.

После этого транзакция переходит в статус «active».

Дальнейший алгоритм действий зависит от уровня изоляции и конкуренции транзакции:

-   для транзакций с пессимистической блокировкой транзакция сразу проверяет возможность получить блокировку по ключу;
-   для транзакций с оптимистической блокировкой проверка возможности получения блокировки по ключу происходит при выполнении операции `commit`.

:::{admonition} Примечание
:class: note

Все выполняемые `get`-операции (в режиме Repeatable Read) приводят к чтению данных из кластера и сохранению ключей и значений в контексте транзакции в java heap NearNode.

Все выполняемые `put`-операции также приводят к кешированию ключей и данных в контексте транзакции в java heap NearNode.
:::

##### Отмена транзакции (TX Rollback)

В случае отмены транзакции приложением (выполнение `tx.rollback`) DataGrid ведет себя следующим образом:

- при пессимистической блокировке — уберет полученные блокировки и удалит контекст транзакции;
- при оптимистической блокировке — удалит контекст транзакции на NearNode, так как блокировки приобретаются только в момент выполнения операции `commit`.

##### TX Timeout

DataGrid позволяет задать параметр `timeout` для транзакций. Это позволяет отменить транзакцию, если она превысит указанное время выполнения.

Проверка соответствия фактического времени выполнения транзакции заданному времени выполняется на каждом Primary-узле при попытках:

-   получить блокировку на ключ (в пессимистическом режиме);
-   начать фазу prepare (в оптимистическом режиме, либо при наличии всех необходимых блокировок для пессимистического режима).

Если время выполнения транзакции превысило заданное на любом из узлов, для транзакции выставляется статус, означающий, что она не может быть сохранена, а может быть только отменена. Эта информация отправляется на NearNode. В случае если NearNode получил транзакцию с таким признаком, то для нее выполняется обычная процедура отмены.

##### Фиксация транзакции (TX Commit)

Если пользователь фиксирует транзакцию (`tx.commit`), то на основании контекста этой транзакции создается запрос на выполнение шага prepare.

При этом на каждый Primary-узел отправляется информация по новым или измененным ключам и информация о порядке получения блокировок.

Primary-узлы выполняют следующие функции:

-   проверяют, что версии топологии транзакции и узла совпадают;
-   получают необходимые блокировки;
-   создают DHT-контекст для транзакции и сохраняют в него необходимые данные;
-   в зависимости от режима, в котором работает кеш, дожидаются подтверждения успешности шага `prepare` от резервных (backup) узлов, либо игнорируют этот шаг;
-   информируют NearNode о готовности выполнить операцию `commit`.

Затем NearNode отправляет коммит-сообщение, дожидается подтверждения и переводит транзакцию в статус `COMMITTED`.

:::{admonition} Внимание
:class: danger

Все данные транзакции должны помещаться в heap-памяти NearNode. В противном случае транзакция завершится ошибкой OutOfMemory.
:::

##### Восстановление после сбоев

Для сохранения работоспособности кластера, особенно в системах с большим числом участников, необходимы механизмы предотвращения или исправления отказов любого из элементов.

Ниже приведены механизмы для следующих сценариев:

-   сбои резервного (backup) узла;
-   сбои основного (primary) узла в фазе `prepare`;
-   сбои основного узла в фазе `commit`;
-   сбои узла-координатора.

##### Сбои резервного (backup) узла

В случае отказа резервного узла в фазах `prepare` или `commit` специальных действий не требуется.

Данные сохраняются на основных и оставшихся резервных узлах, после чего, вне транзакции, для партиции создается новая резервная копия.

##### Сбои основного (primary) узла в фазе `prepare`

Для исправления сбоя основного узла действует следующий алгоритм: если сбой произошел перед началом или во время стадии `prepare`, и координатор транзакции не смог подключиться к основному узлу, он инициирует исключение, и клиент должен принять решение (отмена или повторная попытка сохранить результаты транзакции).

:::{admonition} Примечание
:class: note

Повторная попытка сохранения результатов выполняется в новой топологии и, соответственно, в новой транзакции.
:::

##### Сбои основного (primary) узла в фазе commit

Если отказ произошел после завершения фазы `prepare`, координатор транзакции, определив недоступность основного узла (`NodeFailureDetection`), будет ожидать ответа от резервных узлов.

Резервные узлы, определив отключение основного узла, отправляют сообщение координатору транзакции с подтверждением успешного сохранения изменений и отсутствия потерь данных, благодаря наличию дополнительной резервной копии данных для использования приложением. Информация об инициаторе и всех участниках транзакции включена в сообщение, полученное в фазе `prepare`.

:::{admonition} Примечание
:class: note

Топология кластера при этом не меняется. Для смены топологии необходимо прохождение сообщения PME, а оно будет ожидать окончания активных транзакций.
:::

Затем происходит подтверждение транзакции координатором, транзакция перестает быть активной, изменяется топология, и кластер выберет новый основной узел для партиций, сохраненных на предыдущем основном узле.

##### Сбои узла-координатора

Каждый участник транзакции располагает информацией только о локальном статусе транзакции. Поэтому все основные узлы отправляют сообщения с информаций о текущем статусе. Если один из узлов ответит, что prepare-сообщение к нему не поступало (или не ответит вообще), то транзакция отменяется на всех узлах.
